# DR setup for Docker
## Step 1: First create SSH key on DR server
```
ssh-keygen -t ed25519
```
## Step 2: Setup User In DC server

We create user name drsync  
```
sudo useradd -m -s /bin/bash drsync 2>/dev/null || true
sudo passwd -l drsync

sudo mkdir -p /home/drsync/.ssh
sudo chmod 700 /home/drsync/.ssh
sudo touch /home/drsync/.ssh/authorized_keys
sudo chmod 600 /home/drsync/.ssh/authorized_keys
sudo chown -R drsync:drsync /home/drsync/.ssh
```
## Step 2.1: Allow drsync to run rsync as root (passwordless) on DC:
type
```
visudo 
```
and 
```
drsync ALL=(root) NOPASSWD:/usr/bin/rsync
```
## Step 3: Paste DR ssh key to DC server
```
echo 'ssh-key' \
| sudo tee -a /home/drsync/.ssh/authorized_keys >/dev/null
sudo chown drsync:drsync /home/drsync/.ssh/authorized_keys
sudo chmod 600 /home/drsync/.ssh/authorized_keys
sudo systemctl restart sshd
```
### Give drsync read-only access to contentstore
```
# 1) Traverse-only on parent dirs (no listing)
sudo setfacl -m u:drsync:--x /root
sudo setfacl -m u:drsync:--x /root/volumes
sudo setfacl -m u:drsync:--x /root/volumes/data
sudo setfacl -m u:drsync:--x /root/volumes/data/alf-repo-data

# 2) Contentstore: read + traverse (and defaults for new files/dirs)
sudo setfacl -R -m u:drsync:rx /root/volumes/data/alf-repo-data/contentstore
sudo setfacl -R -d -m u:drsync:rx /root/volumes/data/alf-repo-data/contentstore

# 3) contentstore.deleted (optional)
sudo setfacl -R -m u:drsync:rx /root/volumes/data/alf-repo-data/contentstore.deleted 2>/dev/null || true
sudo setfacl -R -d -m u:drsync:rx /root/volumes/data/alf-repo-data/contentstore.deleted 2>/dev/null || true

# 4) WAL archive: read + traverse (and defaults for new WAL files)
sudo setfacl -R -m u:drsync:rx /root/volumes/data/wal-archive
sudo setfacl -R -d -m u:drsync:rx /root/volumes/data/wal-archive

```

#### On DC add sudoers rule for rsync user
```
sudo tee /etc/sudoers.d/drsync-rsync >/dev/null <<'EOF'
drsync ALL=(root) NOPASSWD: /usr/bin/rsync
Defaults:drsync !requiretty
EOF
sudo chmod 440 /etc/sudoers.d/drsync-rsync
```
#### Ensure the remote Permission
```
chmod 700 /home/drsync/.ssh
chmod 600 /home/drsync/.ssh/authorized_keys
```
#### Test the connectivity from DR Server
```
ssh -i /home/ketan_gcloud/.ssh/id_ed25519 drsync@10.128.0.13 "whoami; hostname"
```
```
ssh -i /root/.ssh/id_ed25519 drsync@10.128.0.18 "ls -ld /home/ketanexam92; ls -l /home/ketanexam92 | head"
```
PostgreSQL Streaming Replication Setup (Docker Compose Alfresco)
================================================================

Overview
--------

*   **DC (Primary)**: Accepts writes from Alfresco.
*   **DR (Standby)**: Receives WAL continuously from DC using **physical streaming replication**.
*   DR database stays **read-only** while in recovery (`pg_is_in_recovery() = true`).
*   We used a **physical replication slot** `dr_slot` so WAL isn’t removed before DR receives it.



### 1. Create replicator role (idempotent)
```
docker compose exec -T postgres psql -U alfresco -d postgres -v ON_ERROR_STOP=1 -c "DO \$\$ BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname='replicator') THEN
    CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'StrongPass@123';
  END IF;
END \$\$;"
```

### 2. Create physical replication slot dr_slot (idempotent)

```
docker compose exec -T postgres psql -U alfresco -d postgres -v ON_ERROR_STOP=1 -c "DO \$\$ BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_replication_slots WHERE slot_name='dr_slot') THEN
    PERFORM pg_create_physical_replication_slot('dr_slot');
  END IF;
END \$\$;"

```
## Verification On DC
```
docker compose exec -T postgres psql -U alfresco -d postgres -c "SELECT rolname, rolreplication, rolcanlogin FROM pg_roles WHERE rolname='replicator';"
docker compose exec -T postgres psql -U alfresco -d postgres -c "SELECT slot_name, slot_type, active FROM pg_replication_slots WHERE slot_name='dr_slot';"

```
On DC:

```bash
docker compose exec -T postgres psql -U alfresco -d postgres -c \
"select client_addr,state,sync_state,write_lag,flush_lag,replay_lag from pg_stat_replication;"
```

Expected:

*   `client_addr = 10.160.0.3`
*   `state = streaming`

* * *

B) DR (Standby) PostgreSQL Configuration
========================================

B1) Prepare standby with base backup from DC (critical step)
------------------------------------------------------------

We stopped DR postgres and ran `pg_basebackup` **from DR** to pull the database from DC.

Command (run on DR):

```bash
set -a; source .env; set +a
PRIMARY_IP="10.128.0.4" #DC sever ip
export REPL_PASSWORD='StrongPass@123'

docker compose stop postgres

docker compose run --rm \
  -e PGPASSWORD="$REPL_PASSWORD" \
  postgres bash -lc "
set -e
rm -rf \"\$PGDATA\"/*
pg_basebackup -h $PRIMARY_IP -p 5432 -U replicator -D \"\$PGDATA\" -Fp -Xs -P -R -S dr_slot
echo \"primary_slot_name = 'dr_slot'\" >> \"\$PGDATA/postgresql.auto.conf\"
"
docker compose up -d postgres
```

**What this did:**

*   `rm -rf "$PGDATA"/*` → cleans old standby data
*   `pg_basebackup`:
    *   copies a consistent base backup from DC
    *   `-R` writes standby config automatically (creates `standby.signal` + `primary_conninfo`)
    *   `-S dr_slot` uses the DC replication slot
*   `primary_slot_name='dr_slot'` ensures standby uses the slot.

* * *

B2) Standby-side verification (DR)
----------------------------------

On DR:

```bash
docker compose exec postgres psql -U alfresco -d template1 -c "SELECT pg_is_in_recovery();"
docker compose exec postgres psql -U alfresco -d template1 -c "SELECT status, sender_host FROM pg_stat_wal_receiver;"
```

Expected:

*   `pg_is_in_recovery()` = `t`
*   `pg_stat_wal_receiver.status` = `streaming`
*   `sender_host` = `10.128.0.4`
*   `slot_name` = `dr_slot`

* * *

C) Common Issues We Hit (and Fixes)
===================================

C1) `FATAL: role "root" does not exist`
---------------------------------------

Cause: `.env` not loaded → `$POSTGRES_USER` empty.

Fix:

```bash
set -a; source .env; set +a
```

* * *

C2) DR query column mismatch (`received_lsn does not exist`)
------------------------------------------------------------

Cause: PostgreSQL version differences.  
Fix: Use columns that exist in your PG16 view:

*   `written_lsn`, `flushed_lsn`, `latest_end_lsn`.

* * *

D) Final Health Check Commands (for documentation)
==================================================

DC checks
---------

```bash
set -a; source .env; set +a
docker compose exec -T postgres psql -U "$POSTGRES_USER" -d postgres -c \
"select client_addr,state,sync_state,write_lag,flush_lag,replay_lag from pg_stat_replication;"
```

DR checks
---------

```bash
set -a; source .env; set +a
docker compose exec -T postgres psql -U "$POSTGRES_USER" -d postgres -c "select pg_is_in_recovery();"
docker compose exec -T postgres psql -U "$POSTGRES_USER" -d postgres -c "select * from pg_stat_wal_receiver;"
```

* * *

If you want, paste your **DC** and **DR** `postgres:` service sections from docker-compose (just that block), and I’ll convert this into a polished “SOP” format (Prereqs → Commands → Expected Output → Rollback → Failover/Failback notes).

# Create cron Jobs for Rsync 
## 1: Create Script for rsync -
**for DR**
```
#!/usr/bin/env bash
set -euo pipefail
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# DR is PRIMARY, push data from DR -> DC
DC_IP="10.128.0.17"
SSH_USER="drsync"
SSH_KEY="/root/.ssh/id_ed25519"
SSH_OPTS="-i ${SSH_KEY} -o StrictHostKeyChecking=accept-new"

# Local source on DR
BASE_LOCAL="/root/volumes/data/alf-repo-data"
# Remote destination base on DC
BASE_REMOTE="${SSH_USER}@${DC_IP}:${BASE_LOCAL}"

# Logs/lock stored in ./logs directory next to this script
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_DIR="${SCRIPT_DIR}/logs"
LOG="${LOG_DIR}/alf-contentstore-sync.log"
LOCK="${LOG_DIR}/alf-contentstore-sync.lock"

mkdir -p "$LOG_DIR"

# prevent overlapping runs
exec 200>"$LOCK"
flock -n 200 || exit 0

ts() { date -Is; }

echo "[$(ts)] START alfresco content sync (DR -> DC)" >> "$LOG"

# Common rsync options (safe defaults: NO --delete for continuous sync)
RSYNC_OPTS=(-aHAX --numeric-ids --partial --info=progress2)

sync_dir () {
  local dir="$1"
  local src="${BASE_LOCAL}/${dir}/"
  local dst="${BASE_REMOTE}/${dir}/"

  if [[ ! -d "${BASE_LOCAL}/${dir}" ]]; then
    echo "[$(ts)] SKIP ${dir} (not found on DR)" >> "$LOG"
    return 0
  fi

  echo "[$(ts)] RSYNC ${dir}" >> "$LOG"
  rsync "${RSYNC_OPTS[@]}" -e "ssh ${SSH_OPTS}" --rsync-path="sudo -n rsync" "$src" "$dst" >> "$LOG" 2>&1
}

sync_dir "contentstore"
sync_dir "contentstore.deleted"


rc=$?
echo "[$(ts)] END alfresco content sync (rc=$rc)" >> "$LOG"
exit $rc
```

**For DC**
```
#!/usr/bin/env bash
set -euo pipefail
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# DR is PRIMARY, push data from DR -> DC
DR_IP="10.128.0.18"
SSH_USER="drsync"
SSH_KEY="/root/.ssh/id_ed25519"
SSH_OPTS="-i ${SSH_KEY} -o StrictHostKeyChecking=accept-new"

# Local source on DR
BASE_LOCAL="/root/volumes/data/alf-repo-data"
# Remote destination base on DC
BASE_REMOTE="${SSH_USER}@${DR_IP}:${BASE_LOCAL}"

# Logs/lock stored in ./logs directory next to this script
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_DIR="${SCRIPT_DIR}/logs"
LOG="${LOG_DIR}/alf-contentstore-sync.log"
LOCK="${LOG_DIR}/alf-contentstore-sync.lock"

mkdir -p "$LOG_DIR"

# prevent overlapping runs
exec 200>"$LOCK"
flock -n 200 || exit 0

ts() { date -Is; }

echo "[$(ts)] START alfresco content sync (DR -> DC)" >> "$LOG"

# Common rsync options (safe defaults: NO --delete for continuous sync)
RSYNC_OPTS=(-aHAX --numeric-ids --partial --info=progress2)

sync_dir () {
  local dir="$1"
  local src="${BASE_LOCAL}/${dir}/"
  local dst="${BASE_REMOTE}/${dir}/"

  if [[ ! -d "${BASE_LOCAL}/${dir}" ]]; then
    echo "[$(ts)] SKIP ${dir} (not found on DR)" >> "$LOG"
    return 0
  fi

  echo "[$(ts)] RSYNC ${dir}" >> "$LOG"
  rsync "${RSYNC_OPTS[@]}" -e "ssh ${SSH_OPTS}" --rsync-path="sudo -n rsync"  "$src" "$dst" >> "$LOG" 2>&1
}

sync_dir "contentstore"
sync_dir "contentstore.deleted"


rc=$?
echo "[$(ts)] END alfresco content sync (rc=$rc)" >> "$LOG"
exit $rc

```
### 1.2: test 
```
/usr/local/bin/alf-contentstore-sync.sh
tail -n 50 /var/log/alf-contentstore-sync.log
```
### 1.3 Create cronTab 
```
sudo crontab -e
```
Add (every 10 minutes):
```
*/10 * * * * /usr/local/bin/alf-contentstore-sync.sh
```

Check cron logs:
```
sudo systemctl enable --now crond
sudo tail -n 50 /var/log/cron
```

## 2: WAL archive rsync cron (DR pulls from DC)
### 2.1 Create script on DR
```
sudo tee /usr/local/bin/pg-wal-archive-sync.sh >/dev/null <<'EOF'
#!/usr/bin/env bash
set -euo pipefail

DC_IP="10.128.0.17"
SRC="drsync@${DC_IP}:/root/volumes/data/wal-archive/"
DEST="/root/volumes/data/wal-archive/"

LOG="/var/log/pg-wal-archive-sync.log"
LOCK="/var/lock/pg-wal-archive-sync.lock"

mkdir -p "$(dirname "$LOG")" "$(dirname "$LOCK")" "$DEST"

exec 200>"$LOCK"
flock -n 200 || exit 0

echo "[$(date -Is)] START wal-archive rsync" >> "$LOG"

# NOTE: no --delete by default (safer for WAL archive)
rsync -aHAX --numeric-ids --inplace --partial --info=progress2 \
  -e "ssh -i /root/.ssh/id_ed25519 -o StrictHostKeyChecking=accept-new" \
  --rsync-path="sudo -n rsync" \
  "$SRC" "$DEST" >> "$LOG" 2>&1

echo "[$(date -Is)] END wal-archive rsync (rc=$?)" >> "$LOG"
EOF

sudo chmod +x /usr/local/bin/pg-wal-archive-sync.sh
```

### 2.2 test
```
sudo /usr/local/bin/pg-wal-archive-sync.sh
tail -n 50 /var/log/pg-wal-archive-sync.log
```
### 2.3 Add cron entry on DR
Edit root crontab again:
```
sudo crontab -e
```

Add (every 1 minute — good for low RPO):
```
* * * * * /usr/local/bin/pg-wal-archive-sync.sh
```
